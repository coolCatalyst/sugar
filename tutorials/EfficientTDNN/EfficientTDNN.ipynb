{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "similar-commodity",
   "metadata": {},
   "source": [
    "# EfficientTDNN\n",
    "\n",
    "This tutorial aims to show how to load a subnet from the trained supernet and evalute is on several test sets. Taking the size as the same as the mentioned in [ECAPA-TDNN](http://www.isca-speech.org/archive/Interspeech_2020/abstracts/2650.html) or Efficient-Base in [EfficientTDNN](https://arxiv.org/abs/2103.13581), the implementation details are summarized as follows.\n",
    "\n",
    "1. Prepare the weights of the supernet and the batchnorm of a subnet.\n",
    "2. Define the architecture of the subnet and load the weights.\n",
    "3. Profile the efficiency metrics of the subnet, such as memory.\n",
    "4. Evaluate the subnet in EER and minDCF $_{0.01}$ on several test sets, such as Vox1-O, Vox1-E, Vox1-H."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sugar.models import WrappedModel, veri_validate\n",
    "from sugar.database import Utterance\n",
    "from sugar.data.voxceleb1 import veriset\n",
    "from sugar.scores import score_cohorts, asnorm\n",
    "from sugar.vectors import extract_vectors\n",
    "from sugar.metrics import print_size_of_model, profile, latency, calculate_mindcf, calculate_eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80240743",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e216e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We have the supernet denoted as with the largest scale denoted as `(4, [512, 512, 512, 512, 512], [5, 5, 5, 5, 5], 1536)`. In the supernet, different kernels are transformed between each other via linear transformation matrics. For different progressive training stages, the trained supernet can be concluded as follows.\n",
    "\n",
    "- largest: a single network with the maximum architecture in the supernet.\n",
    "- kernel: 243 subnets that are nested in the supernet with the kernel size `{1, 3, 5}` at different layers.\n",
    "- depth: 351 subnets with the depth `{2, 3, 4}` based on the kernel stage.\n",
    "- width 1: a large number of subnets where the number of channels between `[0.5, 1.0]`.\n",
    "- width 2: a huge number of subnets support the minimum `0.25` channels.\n",
    "\n",
    "\n",
    "The bounded subnets from the supernet are summarized as follows.\n",
    "\n",
    "- largest: `(4, [512, 512, 512, 512, 512], [5, 5, 5, 5, 5], 1536)`\n",
    "- Kmin: `(4, [512, 512, 512, 512, 512], [1, 1, 1, 1, 1], 1536)`\n",
    "- Dmin: `(2, [512, 512, 512], [1, 1, 1], 1536)`\n",
    "- C1min: `(2, [256, 256, 256], [1, 1, 1], 768)`\n",
    "- C2min: `(2, [128, 128, 128], [1, 1, 1], 384)`\n",
    "\n",
    "More details can be found in [EfficientTDNN at arXiv](https://arxiv.org/abs/2103.13581)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-explanation",
   "metadata": {},
   "source": [
    "## Prepare the weights of the supernet and the batchnorm of a subnet and load the subnet\n",
    "\n",
    "- The supernet contains the whole weights including batchnorm and so on.\n",
    "- The weights of batchnorm can denoted as the weights of a subnet, since the other weights of the subnet inherts the supernet but the batchnorm is calibrated by some training speech utterances.\n",
    "\n",
    "Note that the weights are downloaded from [huggingface](https://huggingface.co/mechanicalsea/efficient-tdnn) as follows.\n",
    "\n",
    "- `repo_id = \"mechanicalsea/efficient-tdnn\"`\n",
    "- supernet:\n",
    "  - `filename = \"depth/depth.torchparams\"`\n",
    "- subnet:\n",
    "  - `filename = \"depth/depth-ecapa-tdnn.3.512.512.512.512.5.3.3.3.1536.bn.tar`\n",
    "\n",
    "Specifically, we load the subnet as follows.\n",
    "\n",
    "1. Define the supernet and load the weights of extractors.\n",
    "2. Clone the subnet and load the weights of batchnorms.\n",
    "3. Add the input layer, i.e., log Mel-filterbanks.\n",
    "\n",
    "Note that the head `AAMSoftmax(192, 5994, 0.2, 30)` that serve as computing loss function is not added becasue it do not work for extracting speaker embeddings.\n",
    "\n",
    "We print the results recorded in the paper [EfficientTDNN at arXiv](https://arxiv.org/abs/2103.13581)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b3a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mechanicalsea/efficient-tdnn\"\n",
    "supernet_filename = \"depth/depth.torchparams\"\n",
    "subnet_filename = \"depth/depth.ecapa-tdnn.3.512.512.512.512.5.3.3.3.1536.bn.tar\"\n",
    "subnet, info = WrappedModel.from_pretrained(\n",
    "    repo_id=repo_id, supernet_filename=supernet_filename, subnet_filename=subnet_filename)\n",
    "sup_state_dict = info['supernet']\n",
    "sub_state_dict = info['subnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148f7acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subnet: (3, [512, 512, 512, 512], [5, 3, 3, 3], 1536)\n",
      "Performance:\n",
      "\tEER (%) w/o AS-Norm\t1.14%\n",
      "\tminDCF w/o AS-Norm\t0.106\n",
      "\tEER (%) w/ AS-Norm\t0.94%\n",
      "\tminDCF w/ AS-Norm\t0.089\n"
     ]
    }
   ],
   "source": [
    "print(f\"Subnet: {sub_state_dict['subnet']}\")\n",
    "print(\"Performance:\")\n",
    "for key in sub_state_dict.keys():\n",
    "    if \"EER\" in key or \"minDCF\" in key:\n",
    "        print(f\"\\t{key}\\t{sub_state_dict[key]}{'%' if 'EER' in key else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-triangle",
   "metadata": {},
   "source": [
    "## Profile the efficiency metrics of the subnet\n",
    "\n",
    "After sampling a subnet, the next is to profile its efficiency metrics such as memory, MACs, and parameters, where the MAC is estimated by taking a 3-second utterance as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vulnerable-warner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subnet: (3, [512, 512, 512, 512], [5, 3, 3, 3], 1536)\n",
      "MACs 1.45G Params 5.79M Memory 22.34 MB Latency 11.48 ms on the cuda:3\n"
     ]
    }
   ],
   "source": [
    "input_size = [1, 48000]\n",
    "macs, params = profile(subnet, input_size, device=device)\n",
    "print(f\"Subnet: {sub_state_dict['subnet']}\")\n",
    "model_size = print_size_of_model(subnet)\n",
    "avg_lat = latency(subnet, input_size, device=device)\n",
    "print(f'MACs {macs} Params {params} Memory {model_size:.2f} MB Latency {avg_lat:.2f} ms on the {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-professor",
   "metadata": {},
   "source": [
    "## Evaluate the subnet \n",
    "\n",
    "1. Prepare dataset for evaluation, including cohorts and test set.\n",
    "2. Conduct evaluation on the Vox1-O, Vox1-E, and Vox1-H test set.\n",
    "\n",
    "### Prepare dataset for evalution\n",
    "\n",
    "1. Dataset for evaluating.\n",
    "2. Dataset for cohort-based score normalization.\n",
    "\n",
    "Note that download the VoxCeleb1 and VoxCeleb2 data manually and save in the `vox1_root` and `vox2_root`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b413f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datalst.zip\n",
      "  inflating: list_test_all2.txt      \n",
      "  inflating: list_test_hard2.txt     \n",
      "  inflating: veri_test2.txt          \n",
      "  inflating: vox2.6000.txt           \n",
      "  inflating: vox2_trainlst.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip -o datalst.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b698a5",
   "metadata": {},
   "source": [
    "Set `vox1_root` as the root directory of the VoxCeleb1 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c9f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "vox1_root = \"/workspace/datasets/voxceleb/voxceleb1/\"\n",
    "vox2_root = \"/workspace/datasets/voxceleb/voxceleb2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfc381",
   "metadata": {},
   "source": [
    "Load the test set in the form of verification trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "experienced-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "veritesto = \"veri_test2.txt\"\n",
    "veriteste = \"list_test_all2.txt\"\n",
    "veritesth = \"list_test_hard2.txt\"\n",
    "veri_testo, veri_teste, veri_testh, wav_files = veriset(\n",
    "    test2=veritesto, all2=veriteste, hard2=veritesth, rootdir=vox1_root, num_samples=0, num_eval=1)\n",
    "testo_loader = DataLoader(veri_testo, batch_size=1, shuffle=False, num_workers=0)\n",
    "teste_loader = DataLoader(veri_teste, batch_size=1, shuffle=False, num_workers=0)\n",
    "testh_loader = DataLoader(veri_testh, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f37428",
   "metadata": {},
   "source": [
    "Load cohort dataset as the prepared list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8e3c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract Vectors: 100%|██████████| 6000/6000 [00:51<00:00, 116.11it/s]\n"
     ]
    }
   ],
   "source": [
    "cohort_path = 'vox2.6000.txt'\n",
    "prefix_root = vox2_root\n",
    "with open(cohort_path, 'r') as f:\n",
    "    cohort_txt = f.readlines()\n",
    "    cohortlst = [os.path.join(prefix_root, utt.replace('\\n', '')) for utt in cohort_txt]\n",
    "\n",
    "cohortset = Utterance(cohortlst, num_samples=0, mode_eval=True)\n",
    "cohorts = extract_vectors(subnet, cohortset, device=device)\n",
    "cohorts = torch.cat(list(cohorts.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5169c03a",
   "metadata": {},
   "source": [
    "### Evaluate on the Vox1-O test set\n",
    "\n",
    "- EER and minDCF_${0.01}$ without the cohort-based adaptive score normalization (AS-Norm).\n",
    "- Applying the AS-Norm with the cohort set containing utterance-wise speaker embeddings.\n",
    "\n",
    "Note that the size of cohort set is smaller than that is used in [ECAPA-TDNN](http://www.isca-speech.org/archive/Interspeech_2020/abstracts/2650.html), where all training utterances are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7a70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_veri(test_loader, network, p_target=0.01, device=\"cpu\", vectors=None):\n",
    "    eer, dcf, vec, scs = veri_validate(test_loader, network, p_target=0.01, device=device, ret_info=True, vectors=vectors)\n",
    "    scs = pandas.DataFrame({'score': scs, 'enroll': test_loader.dataset.enrolls, 'test': test_loader.dataset.tests})\n",
    "    labs = test_loader.dataset.labels\n",
    "    eer = eer[0] * 100\n",
    "    dcf = dcf[0]\n",
    "    return eer, dcf, vec, scs\n",
    "\n",
    "def eval_asnorm(labs, vec, scs, cohorts, p_target=0.01):\n",
    "    cohorts_o = score_cohorts(cohorts, vec)\n",
    "    asso = asnorm(scs, cohorts_o)\n",
    "    eer_o_asnorm = calculate_eer(labs, asso)[0] * 100\n",
    "    dcf_o_asnorm = calculate_mindcf(labs, asso, p_target=0.01)[0]\n",
    "    return eer_o_asnorm, dcf_o_asnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65723568",
   "metadata": {},
   "source": [
    "Expected results:\n",
    "\n",
    "|Metric|Result|\n",
    "|:-----|-----:|\n",
    "|EER (%) w/o AS-Norm|\t1.14%|\n",
    "|minDCF w/o AS-Norm|\t0.106|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70dea6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract Vectors: 100%|██████████| 4708/4708 [01:05<00:00, 71.58it/s]\n",
      "Compute Scores: 100%|██████████| 37611/37611 [00:07<00:00, 5300.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on Vox1-O: * EER/DCF 1.14%/0.106\n"
     ]
    }
   ],
   "source": [
    "eero, dcfo, veco, scso = eval_veri(testo_loader, subnet, device=device)\n",
    "print(f'Evaluate on Vox1-O: * EER/DCF {eero:.2f}%/{dcfo:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849a6ea7",
   "metadata": {},
   "source": [
    "AS-Norm improves the performance in both EER and minDCF.\n",
    "\n",
    "Expected results:\n",
    "\n",
    "|Metric|Result|\n",
    "|:-----|-----:|\n",
    "|EER (%) w/o AS-Norm|\t0.94%|\n",
    "|minDCF w/o AS-Norm|\t0.089|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "691aaa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score Cohorts: 100%|██████████| 4708/4708 [00:02<00:00, 1837.86it/s]\n",
      "Cohort Statistics: 100%|██████████| 4708/4708 [00:00<00:00, 5545.38it/s]\n",
      "Normalization Statistics: 100%|██████████| 37611/37611 [00:00<00:00, 284374.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS-Norm on Vox1-O: * EER/DCF 0.94%/0.089\n"
     ]
    }
   ],
   "source": [
    "eer_o_asnorm, dcf_o_asnorm = eval_asnorm(testo_loader.dataset.labels, veco, scso, cohorts, p_target=0.01)\n",
    "print(f'AS-Norm on Vox1-O: * EER/DCF {eer_o_asnorm:.2f}%/{dcf_o_asnorm:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd04436",
   "metadata": {},
   "source": [
    "### Evaluate on the Vox1-E test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d3d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract Vectors:   0%|          | 520/145160 [00:07<34:57, 68.96it/s]"
     ]
    }
   ],
   "source": [
    "eere, dcfe, vece, scse = eval_veri(teste_loader, subnet, device=device)\n",
    "print(f'Evaluate on Vox1-E: * EER/DCF {eere:.2f}%/{dcfe:.3f}')\n",
    "time.sleep(1)\n",
    "eer_e_asnorm, dcf_e_asnorm = eval_asnorm(teste_loader.dataset.labels, vece, scse, cohorts, p_target=0.01)\n",
    "print(f'AS-Norm on Vox1-E: * EER/DCF {eer_e_asnorm:.2f}%/{dcf_e_asnorm:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acc144",
   "metadata": {},
   "source": [
    "### Evaluate on the Vox1-H test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca826e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute Scores: 100%|██████████| 550894/550894 [01:44<00:00, 5290.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on Vox1-H: * EER/DCF 2.41%/0.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Score Cohorts: 100%|██████████| 145160/145160 [02:05<00:00, 1158.00it/s]\n",
      "Cohort Statistics: 100%|██████████| 145160/145160 [00:26<00:00, 5497.97it/s]\n",
      "Normalization Statistics: 100%|██████████| 550894/550894 [00:02<00:00, 249554.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS-Norm on Vox1-H: * EER/DCF 2.18%/0.206\n"
     ]
    }
   ],
   "source": [
    "eerh, dcfh, vech, scsh = eval_veri(testh_loader, subnet, device=device, vectors=vece)\n",
    "print(f'Evaluate on Vox1-H: * EER/DCF {eerh:.2f}%/{dcfh:.3f}')\n",
    "time.sleep(1)\n",
    "eer_h_asnorm, dcf_h_asnorm = eval_asnorm(testh_loader.dataset.labels, vech, scsh, cohorts, p_target=0.01)\n",
    "print(f'AS-Norm on Vox1-H: * EER/DCF {eer_h_asnorm:.2f}%/{dcf_h_asnorm:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-johnson",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The results of the subnet `(3, [512, 512, 512, 512], [5, 3, 3, 3], 1536)` are summarized as follows.\n",
    "\n",
    "| Architecture | Vox1-O EER(%) | Vox1-O DCF $_{0.01}$ | Vox1-E EER(%) | Vox1-E DCF $_{0.01}$ | Vox1-H EER(%) | Vox1-H DCF $_{0.01}$ |\n",
    "|:----|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| ECAPA-TDNN(512) reported | 1.01 | 0.1274 | 1.24 | 0.1418 | 2.32 | 0.2181 |\n",
    "| EfficientTDNN-Base| 0.94 | 0.089 | 1.20 | 0.131 | 2.18 | 0.206 |\n",
    "\n",
    "To conclude, the tutorial shows that how to evaluate a subnet that inherits from the trained supernet and loads the weights of the subnet, in terms of MACs, parameters, memory, latency, EER, and DCF $_{0.01}$ .\n",
    "\n",
    "The implementation details about the search process can be found in [TDNN-NAS](./TDNN-NAS.ipynb).\n",
    "\n",
    "## Referencing ECAPA-TDNN\n",
    "\n",
    "```\n",
    "@inproceedings{DBLP:conf/interspeech/DesplanquesTD20,\n",
    "  author    = {Brecht Desplanques and\n",
    "               Jenthe Thienpondt and\n",
    "               Kris Demuynck},\n",
    "  editor    = {Helen Meng and\n",
    "               Bo Xu and\n",
    "               Thomas Fang Zheng},\n",
    "  title     = {{ECAPA-TDNN:} Emphasized Channel Attention, Propagation and Aggregation\n",
    "               in {TDNN} Based Speaker Verification},\n",
    "  booktitle = {Interspeech 2020},\n",
    "  pages     = {3830--3834},\n",
    "  publisher = {{ISCA}},\n",
    "  year      = {2020},\n",
    "}\n",
    "```\n",
    "\n",
    "## Citing EfficientTDNN\n",
    "\n",
    "Please, cite EfficientTDNN if you use it for your research or business.\n",
    "\n",
    "```bibtex\n",
    "@article{speechbrain,\n",
    "  title={{EfficientTDNN}: Efficient Architecture Search for Speaker Recognition in the Wild},\n",
    "  author={Rui Wang and Zhihua Wei and Haoran Duan and Shouling Ji and Zhen Hong},\n",
    "  year={2021},\n",
    "  eprint={2103.13581},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2103.13581}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8505bd93e15232b680218bd613f68dd2d0ec76b40a79f48f6cb2b19121cd32c4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('azureml_py36_pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
